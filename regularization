Why do we need regularization:

If the model is complex , then the overfitting will happen. So we will have high train accuracy but less test accuracy. So we introduce 
regularization to avoid that.

Overfitting:

Overfitting means the model will learn the parameters based on the training sets we provide. It will do well for the training set. But it will
perform very poorly for the test datas. Meaning, the model will fit the parameters strictly for the training set.

* When the lambda ( regularisation term) is high , we will get low variance but high bias.

ADDING NOISE:

* when adding noise to input features, variance is getting reduced.
* high values of noise is giving low variance. very large noise value is giving low variance but high bias.


EARLY STOPPING:

* early stopping is not good for me with "he" and "relu" ,"leaky_relu".


* In our exercise, when we increase the features from 2 to 4, we are getting low bias and low variance without regularization.
* In our exercise , when we add small regularisation term , we are getting very low bias and very low variance. ( with 4 features as above).
But a high regularisation is increasing bias.
* In our exercise , a low noise is giving low bias and low varinace.
